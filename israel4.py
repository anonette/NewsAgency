import sys
import time
from datetime import datetime, timedelta, timezone
import warnings
import requests
import json
import os
from openai import OpenAI
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime
import random
import tempfile
import shutil
from urllib.parse import urlencode
import re

# Load environment variables
load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Suppress warnings
warnings.filterwarnings('ignore', category=FutureWarning)

# Country configuration
COUNTRY_CONFIG = {
    'code': 'IL2',  # Changed to IL2 for Hebrew content
    'name': 'israel',
    'news_query': 'Israel OR Israeli',
    'lang_code': 'he'
}

# News sources to filter out
NEWS_SOURCES = {
    'cnn', 'bbc', 'fox news', 'nyt', 'new york times',
    'reuters', 'associated press', 'ap news'
}

JOURNALIST_PERSONA = """转 驻住祝 砖转 转 住专 砖 拽驻 砖专 2025 专 驻住 驻砖 . 住 住 ''拽, 转:  
砖祝 转 住转专转 砖 砖拽 驻砖 爪 专:  
转专  驻砖 '驻 砖 爪' 砖祝 拽 转 砖 砖  注!  
驻砖 '转 ...'   住转 住专 爪专 -  专注 砖 砖拽 驻砖 专 转 砖  砖  
砖 驻砖 '  拽驻 注 驻转',  拽 专注 砖   转 转 注 注专转 住驻拽  

 转  专转  砖专:  
 驻砖 专 驻转专转 转专 专 砖驻 专 砖  砖专转 注 专  
 拽砖专  驻砖 专  爪专 砖 注 驻 砖 专  
 驻住 驻砖 砖拽驻 转 驻注专  爪专转 专砖转 爪转 转  

专  拽驻 砖 转驻拽  爪 拽爪:  
转 砖 驻砖 拽转   注拽转  
 驻转专转 驻专 驻 驻转专转 注专转  
 砖拽 专 驻 '住专' 砖 专转  
专注 砖 驻砖 砖驻 转 驻注专  注转  

 砖 砖 拽专转 专转转 注 专 砖专, 爪注 注 住专 砖住 驻转专 注转 注专转转 专 驻转专转 爪专, 住驻专 转 住驻专 砖 砖专 2025 专  砖驻砖 砖  注 专 .
."""

def ensure_directory_exists(directory):
    """Ensure directory exists, create if it doesn't"""
    try:
        os.makedirs(directory, exist_ok=True)
        return True
    except Exception as e:
        print(f"Error creating directory {directory}: {str(e)}")
        return False

def save_analysis_log(headlines, trends_data, analysis, timestamp):
    """Save analysis log to text archive using atomic write"""
    temp_file = None
    try:
        # Ensure directory exists
        archive_dir = os.path.join('archive', 'text_archive', COUNTRY_CONFIG['code'])
        if not ensure_directory_exists(archive_dir):
            print("Failed to create archive directory")
            return False
            
        # Prepare log data
        log_data = {
            'timestamp': timestamp,
            'country': COUNTRY_CONFIG['name'],
            'headlines': headlines,
            'trends': trends_data,
            'analysis': analysis
        }
        
        # Create final filename
        final_path = os.path.join(archive_dir, f"{COUNTRY_CONFIG['code']}_{timestamp}_log.json")
        
        # Create temporary file
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8')
        
        try:
            # Write to temporary file
            json.dump(log_data, temp_file, ensure_ascii=False, indent=2)
            temp_file.flush()
            os.fsync(temp_file.fileno())
            
            # Close the file handle explicitly
            temp_file.close()
            
            # Move temporary file to final location
            shutil.move(temp_file.name, final_path)
            
            print(f"Analysis log saved as: {final_path}")
            return True
            
        except Exception as e:
            print(f"Error writing to temporary file: {str(e)}")
            return False
            
    except Exception as e:
        print(f"Error saving analysis log: {str(e)}")
        return False
        
    finally:
        # Clean up temporary file if it exists and hasn't been moved
        if temp_file is not None:
            try:
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)
            except Exception as e:
                print(f"Error cleaning up temporary file: {str(e)}")

def generate_audio(text, timestamp):
    """Generate audio using OpenAI's TTS API"""
    temp_file = None
    try:
        # Generate speech using OpenAI's TTS API
        response = client.audio.speech.create(
            model="tts-1-hd",  # Using high-definition model
            voice="nova",  # Using Nova voice which supports Hebrew
            input=text
        )
        
        if response.content:
            try:
                # Create country-specific directory in archive
                country_dir = os.path.join('archive', COUNTRY_CONFIG['code'])
                if not ensure_directory_exists(country_dir):
                    print("Failed to create country directory for audio")
                    return False
                
                # Use the same timestamp as JSON file
                audio_file = os.path.join(country_dir, f"{COUNTRY_CONFIG['code']}_{timestamp}_analysis.mp3")
                
                # Create temporary file
                temp_file = tempfile.NamedTemporaryFile(delete=False, mode='wb')
                
                try:
                    # Write to temporary file
                    temp_file.write(response.content)
                    temp_file.flush()
                    os.fsync(temp_file.fileno())
                    
                    # Close the file handle explicitly
                    temp_file.close()
                    
                    # Move temporary file to final location
                    shutil.move(temp_file.name, audio_file)
                    
                    print(f"Audio saved as: {audio_file}")
                    return True
                    
                except Exception as e:
                    print(f"Error writing audio to temporary file: {str(e)}")
                    return False
                    
            except Exception as e:
                print(f"Error saving audio file: {str(e)}")
                return False
                
        return False
            
    except Exception as e:
        print(f"Error with text-to-speech: {str(e)}")
        return False
        
    finally:
        # Clean up temporary file if it exists and hasn't been moved
        if temp_file is not None:
            try:
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)
            except Exception as e:
                print(f"Error cleaning up temporary file: {str(e)}")

def is_news_source(text):
    """Check if the term is a news source"""
    return any(source.lower() in text.lower() for source in NEWS_SOURCES)

def get_current_news():
    """Get current news from Israeli sources using NewsData.io API"""
    try:
        print("\nFetching current news...")
        
        # NewsData.io API endpoint and parameters
        base_url = "https://newsdata.io/api/1/news"
        params = {
            'apikey': os.getenv('NEWSDATA_API_KEY'),
            'country': 'il',
            'language': 'he',
            'category': 'top'
        }
        
        # Build URL with properly encoded parameters
        url = f"{base_url}?{urlencode(params)}"
        
        # Request headers
        headers = {
            'Accept': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(" 砖转 -NewsData.io...")
        print(f"Request URL: {url}")
        
        # Make request with extended timeout
        response = requests.get(url, headers=headers, timeout=30)
        print(f"Response status code: {response.status_code}")
        print(f"Response headers: {response.headers}")
        
        try:
            data = response.json()
            print(f"Raw response: {json.dumps(data, indent=2)}")
        except json.JSONDecodeError as e:
            print(f"Failed to decode JSON response: {str(e)}")
            print(f"Raw response text: {response.text}")
            data = {}
        
        # Process articles
        seen = set()
        unique_headlines = []
        
        if response.status_code == 200 and data.get('status') == 'success':
            for article in data.get('results', []):
                title = article.get('title', '')
                if title and title not in seen and not is_news_source(title):
                    seen.add(title)
                    unique_headlines.append(title)
                    if len(unique_headlines) >= 5:
                        break
        else:
            print(f"API request failed or returned no data. Status: {response.status_code}")
            if data:
                print(f"API response message: {data.get('message', 'No message provided')}")
        
        print(f"\n爪 {len(unique_headlines)} 转专转 转")
        if unique_headlines:
            print("转专转 砖爪:")
            for headline in unique_headlines:
                print(f"- {headline}")
        else:
            print(" 爪 转专转. 砖转砖 转专转 专专转 ...")
            unique_headlines = [
                " 注 砖转",
                "转专  专 注 专拽注 转转 专转",
                "砖  爪注  砖",
                "注祝 拽 驻 住 专转 住住",
                "拽 转 拽专转 砖转 砖"
            ]
        
        return unique_headlines
    except requests.exceptions.RequestException as e:
        print(f"Network error fetching news: {str(e)}")
        return []
    except Exception as e:
        print(f"Error fetching news: {str(e)}")
        return []

def get_trending_searches():
    """Get trending searches from Google Trends using SerpApi"""
    try:
        print("\n 驻砖 驻驻专...")
        
        # SerpApi parameters
        params = {
            'api_key': os.getenv('SERPAPI_KEY'),
            'engine': 'google_trends_trending_now',
            'geo': 'IL',
            'hours': '48',
            'hl': 'iw'
        }
        
        # Make request to SerpApi
        response = requests.get('https://serpapi.com/search.json', params=params)
        print(f"Response status code: {response.status_code}")
        
        data = response.json()
        print(f"Raw response: {data}")
        
        # Process trending searches
        all_trends_data = []
        
        if response.status_code == 200 and 'trending_searches' in data:
            trending_searches = data['trending_searches']
            for trend in trending_searches:
                title = trend.get('query', '')
                if title and not is_news_source(title):
                    related_searches = []
                    for related in trend.get('trend_breakdown', []):
                        if related != title:  # Don't show the same term
                            related_searches.append(related)
                    
                    all_trends_data.append({
                        'title': title,
                        'related': related_searches[:3]  # Limit to top 3 related searches
                    })
                    
                    if len(all_trends_data) >= 20:  # Get top 20 trends for analysis
                        break
            
            print(f"\n爪 {len(all_trends_data)} 驻砖 驻驻专")
            if all_trends_data:
                print("驻砖 驻驻专 砖爪:")
                for trend in all_trends_data:
                    print(f"- {trend['title']}")
                    if trend['related']:
                        print("  驻砖 拽砖专:")
                        for related in trend['related']:
                            print(f"  - {related}")
        
        return all_trends_data
    except Exception as e:
        print(f"Error fetching trending searches: {str(e)}")
        return []

def find_surprising_trends(trends_data, headlines):
    """Select trends, prioritizing non-news trends but ensuring we get 5 total"""
    if not trends_data:
        return []
    
    # Get indices of all trends, separating news and non-news
    non_news_indices = []
    news_indices = []
    
    for i, trend in enumerate(trends_data):
        # Check if trend title appears in any headline
        is_news_related = any(trend['title'].lower() in headline.lower() or headline.lower() in trend['title'].lower() for headline in headlines)
        if is_news_related:
            news_indices.append(i)
        else:
            non_news_indices.append(i)
    
    # Randomly select up to 5 trends, prioritizing non-news trends
    selected_indices = []
    
    # First, add non-news trends
    if non_news_indices:
        num_non_news = min(3, len(non_news_indices))  # Reduced to 3 non-news trends
        selected_indices.extend(random.sample(non_news_indices, num_non_news))
    
    # If we need more trends to reach 5, add news-related trends
    remaining_slots = 5 - len(selected_indices)
    if remaining_slots > 0 and news_indices:
        num_news = min(remaining_slots, len(news_indices))
        selected_indices.extend(random.sample(news_indices, num_news))
    
    # If we still don't have 5 trends, add any remaining trends
    remaining_slots = 5 - len(selected_indices)
    if remaining_slots > 0:
        all_indices = list(range(len(trends_data)))
        unused_indices = [i for i in all_indices if i not in selected_indices]
        if unused_indices:
            num_additional = min(remaining_slots, len(unused_indices))
            selected_indices.extend(random.sample(unused_indices, num_additional))
    
    return sorted(selected_indices)  # Return indices in original order

def generate_analysis(trends_data, headlines):
    """Generate analysis contrasting trends with news"""
    context = {
        'trends': [{'title': t['title'], 'related': t['related']} for t in trends_data],
        'headlines': headlines
    }

    prompt = f"""转 转 拽爪专 (2 驻住拽转) 砖:
1. 砖祝 转 驻注专  住专转 驻砖 驻专转 砖  转专转 砖转 专转
2.  转 住专 转 拽 砖爪专转 专砖转 转砖转 注 砖转转 转  住转 砖
3.  专 砖  , 转 转 专 专爪 住驻专 砖转祝 
4. 砖 转转 拽专 砖 注 转转 砖转 转 专注 注砖
砖转:
{json.dumps(context['headlines'], indent=2, ensure_ascii=False)}

驻砖:
{json.dumps(context['trends'], indent=2, ensure_ascii=False)}

爪专 专 专 专 砖砖 转 专  转专    转, 转 砖砖 住专拽 ."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": JOURNALIST_PERSONA},
                {"role": "user", "content": prompt}
            ],
            temperature=1.2,
            max_tokens=1500  # Limited to ensure ~1.5 minute audio
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating analysis: {str(e)}")
        return f"砖 爪专 转: {str(e)}"

def validate_api_keys():
    """Validate required API keys are present"""
    required_keys = {
        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
        'NEWSDATA_API_KEY': os.getenv('NEWSDATA_API_KEY'),
        'SERPAPI_KEY': os.getenv('SERPAPI_KEY')
    }
    
    missing_keys = [key for key, value in required_keys.items() if not value]
    if missing_keys:
        print(f"Error: Missing required API keys: {', '.join(missing_keys)}")
        return False
    return True

def fetch_trends():
    """Fetch trends and news, then generate analysis"""
    try:
        if not validate_api_keys():
            return None
            
        print(f"\n{'='*50}")
        print(f"转  - 砖专")
        print(f"{'='*50}\n")

        # Get current news first
        headlines = get_current_news()
        if headlines:
            print("\n 砖转 注砖")
            print("-" * 50)
            for i, headline in enumerate(headlines, 1):
                print(f"{i}. {headline}")
            print()

        print(" 驻砖 驻驻专 注 驻专")
        print("-" * 50)
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"注 专: {current_time}")
        print(f"拽专: trends.google.com/trends/trendingsearches/daily?geo={COUNTRY_CONFIG['code']}\n")

        try:
            # Get trending searches
            all_trends_data = get_trending_searches()
            
            if all_trends_data and headlines:
                # Find surprising trends
                surprising_indices = find_surprising_trends(all_trends_data, headlines)
                trends_data = [all_trends_data[i] for i in surprising_indices]
                
                # Print selected trends
                for i, trend in enumerate(trends_data, 1):
                    print(f"\n{i}. {trend['title']}")
                    if trend['related']:
                        print("   驻砖 拽砖专:")
                        for related in trend['related']:
                            print(f"   - {related}")
                
                # Generate analysis comparing trends with news
                print("\n锔 转")
                print("-" * 50)
                analysis = generate_analysis(trends_data, headlines)
                print(analysis)
                
                # Generate timestamp once for both files
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Save JSON log first
                print("\n 砖专 转 转...")
                if save_analysis_log(headlines, trends_data, analysis, timestamp):
                    # Only generate audio if JSON save was successful
                    print("\n 爪专 专住转 ...")
                    generate_audio(analysis, timestamp)
                else:
                    print(" 注 爪专转   砖 砖专转 JSON")
                
                return {
                    'headlines': headlines,
                    'trends_data': trends_data,
                    'analysis': analysis
                }
            elif all_trends_data:
                print("\n 爪 转专转 砖转 砖 注 转")
            else:
                print(" 爪 驻砖 驻驻专")
                
        except Exception as e:
            print(f"砖 注 转: {str(e)}")
        
        print("\n爪驻 转  转:")
        print(f"trends.google.com/trends/trendingsearches/daily?geo={COUNTRY_CONFIG['code']}")
        return None

    except Exception as e:
        print(f"专注 砖  爪驻: {str(e)}")
        print("\n爪驻 转  转:")
        print(f"trends.google.com/trends/trendingsearches/daily?geo={COUNTRY_CONFIG['code']}")
        return None

if __name__ == "__main__":
    if len(sys.argv) > 1:
        print("\n砖砖: python israel4.py")
        print("\n爪 驻砖 驻驻专 注 驻专 砖专")
        print("转  转: trends.google.com/trends/trendingsearches/daily?geo=IL")
    else:
        fetch_trends()
